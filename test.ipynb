{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.transforms as T\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "from src.utils import (\n",
    "    create_dataset, plot_spectrogram,\n",
    "    RandomClip, extract_logmel\n",
    ")\n",
    "from src.datasets import VoxCelebDataModule\n",
    "from src.models import (\n",
    "    SEBlock, SpeakerRecognitionModel, ResNetBlock, build_efficientnetv2,\n",
    "    SEResNetBlock, conv1x1, conv3x3\n",
    ")\n",
    "from torch import nn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from src.resnetse import ResNetSE, SEBasicBlock, ResNetSEV2\n",
    "\n",
    "from src.losses import SubCenterAAMSoftmaxLoss\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import roc_curve, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SelfAttentionPooling(nn.Module):\n",
    "    \"\"\"Implementation of Self Attention Pooling (SAP) as\n",
    "    described in [1]. We used GELU instead of tanh as\n",
    "    non-linearity.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "        [1] W. Cai, J. Chen and M. Li, \"Exploring the Encoding \n",
    "        Layer and Loss Function in End-to-End Speaker and \n",
    "        Language Recognition System\", 2018,\n",
    "        https://arxiv.org/abs/1804.05160\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_mels\n",
    "    ) -> None:\n",
    "        super(SelfAttentionPooling, self).__init__()\n",
    "        self.linear = nn.Linear(n_mels, n_mels)\n",
    "        self.attention = nn.Parameter(\n",
    "            torch.FloatTensor(size=(n_mels, 1))\n",
    "        )\n",
    "        self.gelu = nn.GELU()\n",
    "        self.softmax = nn.Softmax2d()\n",
    "\n",
    "        nn.init.xavier_normal_(self.attention)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = x.permute(0,3,1,2)\n",
    "        h = self.gelu(self.linear(y))\n",
    "        mul = torch.matmul(h, self.attention)\n",
    "        w = self.softmax(mul)\n",
    "        w = w.permute(0,2,3,1)\n",
    "        e = torch.sum(x * w, dim=-1)\n",
    "        \n",
    "        return e\n",
    "\n",
    "class ResNet34SE(SpeakerRecognitionModel):\n",
    "    \"\"\"ResNet34 model, as described in [1]. The\n",
    "    implementation is a simplified and slightly\n",
    "    modified version of the official PyTorch \n",
    "    Vision ResNet.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "        [1] K. He, X. Zhang, S. Ren and J. Sun, \"Deep \n",
    "        Residual Learning for Image Recognition\", 2016 IEEE \n",
    "        Conference on Computer Vision and Pattern Recognition \n",
    "        (CVPR), 2016, pp. 770-778.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_mels=80, **kwargs) -> None:\n",
    "        super(ResNet34SE, self).__init__(**kwargs)\n",
    "\n",
    "        self.current_channels = 64\n",
    "        num_heads = 8\n",
    "        dropout = 0.3\n",
    "        self.attn_expansion = 3\n",
    "\n",
    "        # self.instancenorm   = nn.InstanceNorm2d(n_mels)\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            1, \n",
    "            self.current_channels, \n",
    "            kernel_size=7,\n",
    "            stride=2,\n",
    "            padding=3,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(self.current_channels)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2_x = self._make_sequence(64, num_blocks=3)\n",
    "        self.conv3_x = self._make_sequence(128, num_blocks=4, stride=2)\n",
    "        self.conv4_x = self._make_sequence(256, num_blocks=6, stride=2)\n",
    "        self.conv5_x = self._make_sequence(512, num_blocks=3, stride=2)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.attn_pool = SelfAttentionPooling(self.attn_expansion)\n",
    "        # self.pool1 = SelfAttentionPooling(10)\n",
    "        # self.pool2 = VarSelfAttentionPooling(512, 10)\n",
    "        # self.sp = StatsPoolingLayer()\n",
    "\n",
    "        \"\"\"\n",
    "        outmap_size = int(n_mels/8)\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv1d(256 * outmap_size, 128, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Conv1d(128, 256 * outmap_size, kernel_size=1),\n",
    "            nn.Softmax(dim=2),\n",
    "        )\n",
    "        \n",
    "        self.mha_dim = int(self.current_channels / 32)\n",
    "\n",
    "        self.qkv_proj = nn.Linear(self.mha_dim, 3 * self.mha_dim)\n",
    "        self.mha = nn.MultiheadAttention(\n",
    "            embed_dim=self.mha_dim,\n",
    "            num_heads=num_heads, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(self.mha_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.embeddings = nn.Linear(\n",
    "            self.current_channels * self.attn_expansion, \n",
    "            self.embeddings_dim\n",
    "        )\n",
    "        # self.embeddings = nn.Linear(256 * outmap_size, self.embeddings_dim)\n",
    "        # self.clf = nn.Linear(self.embeddings_dim, self.num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, \n",
    "                    mode=\"fan_out\", \n",
    "                    nonlinearity=\"relu\"\n",
    "                )\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        self._set_optimizers()\n",
    "        self._set_hyperparams()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x = self.instancenorm(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.conv2_x(x)\n",
    "        x = self.conv3_x(x)\n",
    "        x = self.conv4_x(x)\n",
    "        x = self.conv5_x(x)\n",
    "        x = self.attn_pool(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        \n",
    "        # x = x.reshape(x.size()[0],-1,x.size()[-1])\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        w = self.attention(x)\n",
    "        x = torch.sum(x * w, dim=2)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        \"\"\"\n",
    "        # x = self.pool1(x)\n",
    "        # x = self.pool2(x)\n",
    "        # x = self.pool(x)\n",
    "        # x = torch.flatten(x, 1)\n",
    "        \"\"\"\n",
    "        x = x.reshape(x.shape[0],-1,self.mha_dim)\n",
    "\n",
    "        qkv = self.qkv_proj(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        attn, _ = self.mha(query=q, key=k, value=v)\n",
    "        x = x + self.dropout(attn)\n",
    "        x = self.norm1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \"\"\"\n",
    "        # x = self.sp(x)\n",
    "        x = self.embeddings(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def _make_sequence(\n",
    "        self,\n",
    "        out_channels: int,\n",
    "        num_blocks: int,\n",
    "        stride: int = 1\n",
    "    ):\n",
    "        downsample = None\n",
    "\n",
    "        # downsample when we increase the dimension, using\n",
    "        # the 1x1 convolution option, as described in the\n",
    "        # ResNet paper.\n",
    "        if stride != 1 or self.current_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.current_channels, out_channels, stride),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            SEResNetBlock(\n",
    "                in_channels=self.current_channels, \n",
    "                out_channels=out_channels, \n",
    "                stride=stride, \n",
    "                downsample=downsample,\n",
    "                se_ratio=8\n",
    "            )\n",
    "        )\n",
    "        self.current_channels = out_channels\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(\n",
    "                SEResNetBlock(\n",
    "                    in_channels=self.current_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    se_ratio=8\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = ResNet34SE(num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((4,1,80,601))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = resnet(a)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = SelfAttentionPooling(n_mels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 57])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.flatten(\n",
    "    res,\n",
    "    start_dim=2\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 3])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(res).shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3a0b09ceef9b827a17ce91fbca1b0359a993a122e166af5f0b6d31a5625f693"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('tf_p3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
