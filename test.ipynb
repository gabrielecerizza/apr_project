{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.transforms as T\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "from src.utils import (\n",
    "    create_dataset, plot_spectrogram,\n",
    "    RandomClip, extract_logmel\n",
    ")\n",
    "from src.datasets import VoxCelebDataModule\n",
    "from src.models import SEBlock, SpeakerRecognitionModel, ResNetBlock, build_efficientnetv2\n",
    "from torch import nn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from src.resnetse import ResNetSE, SEBasicBlock, ResNetSEV2\n",
    "\n",
    "from src.losses import SubCenterAAMSoftmaxLoss\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import roc_curve, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SERes2Block(nn.Module):\n",
    "    \"\"\"Variant of the SE-Res2Block described in [1]. We\n",
    "    modified the architecture to follow more closely the\n",
    "    Res2Block described in [2], by using 2d convolution.\n",
    "    We also inverted the order of RELU and Batch \n",
    "    Normalization.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "        [1] B. Desplanques et al., \"ECAPA-TDNN: Emphasized \n",
    "        Channel Attention, Propagation and Aggregation TDNN \n",
    "        Based Speaker Verification\", Proc. Interspeech \n",
    "        2020, 2020, pp. 3830-3834.\n",
    "\n",
    "        [2] S.-H. Gao et al., \"Res2Net: A New Multi-Scale \n",
    "        Backbone Architecture\", IEEE Transactions on Pattern \n",
    "        Analysis and Machine Intelligence, vol. 43, no. 2, \n",
    "        2021, pp. 652-662.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_channels: int,\n",
    "        scale: int,\n",
    "        dilation: int\n",
    "    ) -> None:\n",
    "        super(SERes2Block, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.conv1 = nn.Conv2d(n_channels, n_channels, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm2d(n_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        conv_ls = [\n",
    "            nn.Conv2d(\n",
    "                n_channels // scale, \n",
    "                n_channels // scale, \n",
    "                kernel_size=3,\n",
    "                padding=dilation,\n",
    "                dilation=dilation\n",
    "            )\n",
    "            for _ in range(scale - 1)\n",
    "        ]\n",
    "\n",
    "        self.K = nn.ModuleList([nn.Identity()] + conv_ls)\n",
    "        self.bn2 = nn.BatchNorm2d(n_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(n_channels, n_channels, kernel_size=1)\n",
    "        self.bn3 = nn.BatchNorm2d(n_channels)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.se = SEBlock(n_channels)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        out_ls = torch.split(out, out.size(1)//self.scale, dim=1)\n",
    "        y_ls = []\n",
    "        \n",
    "        for idx in range(self.scale):\n",
    "            out_split = out_ls[idx]\n",
    "            k_fun = self.K[idx]\n",
    "            if idx <= 1:\n",
    "                y_ls.append(k_fun(out_split))\n",
    "            else:\n",
    "                prev = y_ls[idx - 1]\n",
    "                y_ls.append(k_fun(out_split + prev))\n",
    "\n",
    "        out = torch.cat(y_ls, dim=1)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.se(out)\n",
    "\n",
    "        return out + x\n",
    "\n",
    "class Var_SERes2Block(SERes2Block):\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        out_ls = torch.split(out, out.size(1)//self.scale, dim=1)\n",
    "        y_ls = []\n",
    "        \n",
    "        for idx in range(self.scale):\n",
    "            out_split = out_ls[idx]\n",
    "            k_fun = self.K[idx]\n",
    "            y_ls.append(k_fun(out_split))\n",
    "\n",
    "        out = torch.cat(y_ls, dim=1)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.se(out)\n",
    "\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class AttentiveStatPooling(nn.Module):\n",
    "    \"\"\"Attentive stat pooling layer, as described in [1].\n",
    "    We provide also an implementation with convolution.\n",
    "    Since the paper worked with MFCC instead of spectrograms,\n",
    "    we averaged the values of mean and std, so that we\n",
    "    could remove one dimension.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "        [1] B. Desplanques et al., \"ECAPA-TDNN: Emphasized \n",
    "        Channel Attention, Propagation and Aggregation TDNN \n",
    "        Based Speaker Verification\", Proc. Interspeech \n",
    "        2020, 2020, pp. 3830-3834. \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_features: int, \n",
    "        latent_features: int,\n",
    "        conv: bool = False\n",
    "    ) -> None:\n",
    "        super(AttentiveStatPooling, self).__init__()\n",
    "        if conv:\n",
    "            self.seq = nn.Sequential(\n",
    "                nn.Conv2d(in_features, latent_features, kernel_size=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(latent_features, in_features, kernel_size=1)\n",
    "            )\n",
    "        else:\n",
    "            self.seq = nn.Sequential(\n",
    "                nn.Linear(in_features, latent_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(latent_features, in_features)\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        attn_weights = F.softmax(self.seq(x), dim=1)\n",
    "        mean = torch.sum(attn_weights * x, dim=2).mean(-1)\n",
    "        std = torch.sum(attn_weights * x ** 2, dim=2).mean(-1) - (mean ** 2)\n",
    "        std = torch.sqrt(std)\n",
    "\n",
    "        return torch.cat([mean, std], dim=1)\n",
    "\n",
    "class Var_ECAPA(SpeakerRecognitionModel):\n",
    "    \"\"\"Variant of the ECAPA-TDNN model described in [1]. We\n",
    "    omit the last batch normalization layer and adopt a\n",
    "    different SE-Res2Block and Attentive Stats pooling\n",
    "    layer. We also work on spectrograms instead of MFCC.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "        [1] B. Desplanques et al., \"ECAPA-TDNN: Emphasized \n",
    "        Channel Attention, Propagation and Aggregation TDNN \n",
    "        Based Speaker Verification\", Proc. Interspeech \n",
    "        2020, 2020, pp. 3830-3834. \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_channels: int = 248,\n",
    "        scale: int = 8, \n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super(Var_ECAPA, self).__init__(**kwargs)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, n_channels, kernel_size=5, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(n_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.se1 = Var_SERes2Block(n_channels, scale, dilation=2)\n",
    "        self.se2 = Var_SERes2Block(n_channels, scale, dilation=3)\n",
    "        self.se3 = Var_SERes2Block(n_channels, scale, dilation=4)\n",
    "        self.conv2 = nn.Conv2d(n_channels * 3, n_channels, kernel_size=1, padding=1)\n",
    "        self.attn_pool = AttentiveStatPooling(n_channels, n_channels // 10, conv=True)\n",
    "        self.bn2 = nn.BatchNorm1d(n_channels * 2)\n",
    "        self.embeddings = nn.Linear(n_channels * 2, self.embeddings_dim)\n",
    "        self.clf = nn.Linear(self.embeddings_dim, self.num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        self._set_optimizers()\n",
    "        self._set_hyperparams()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out_se1 = self.se1(out)\n",
    "        out_se2 = self.se2(out_se1)\n",
    "        out_se3 = self.se3(out_se2)\n",
    "        out = torch.cat([out_se1, out_se2, out_se3], dim=1)\n",
    "        out = self.conv2(out)\n",
    "        out = self.attn_pool(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.embeddings(out)\n",
    "\n",
    "        # if self.training:\n",
    "        #    out = self.clf(out)\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecapa = Var_ECAPA(num_classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((4,1,80,301))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = ecapa(a)\n",
    "res.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3a0b09ceef9b827a17ce91fbca1b0359a993a122e166af5f0b6d31a5625f693"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('tf_p3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
