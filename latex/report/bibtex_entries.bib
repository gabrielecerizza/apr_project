@ARTICLE{kabir2021survey,  author={Kabir, Muhammad Mohsin and Mridha, M. F. and Shin, Jungpil and Jahan, Israt and Ohi, Abu Quwsar},  journal={IEEE Access},   title={A Survey of Speaker Recognition: Fundamental Theories, Recognition Methods and Opportunities},   year={2021},  volume={9},  number={},  pages={79236-79263},  doi={10.1109/ACCESS.2021.3084299}}

@article{nagrani2020voxceleb,
title = {Voxceleb: Large-scale speaker verification in the wild},
journal = {Computer Speech \& Language},
volume = {60},
pages = {101027},
year = {2020},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2019.101027},
author = {Arsha Nagrani and Joon Son Chung and Weidi Xie and Andrew Zisserman},
keywords = {Speaker identification, Speaker verification, Deep learning, Convolutional neural network},
abstract = {The objective of this work is speaker recognition under noisy and unconstrained conditions. We make two key contributions. First, we introduce a very large-scale audio-visual dataset collected from open source media using a fully automated pipeline. Most existing datasets for speaker identification contain samples obtained under quite constrained conditions, and usually require manual annotations, hence are limited in size. We propose a pipeline based on computer vision techniques to create the dataset from open-source media. Our pipeline involves obtaining videos from YouTube; performing active speaker verification using a two-stream synchronization Convolutional Neural Network (CNN), and confirming the identity of the speaker using CNN based facial recognition. We use this pipeline to curate VoxCeleb which contains contains over a million ‘real-world’ utterances from over 6000 speakers. This is several times larger than any publicly available speaker recognition dataset. Second, we develop and compare different CNN architectures with various aggregation methods and training loss functions that can effectively recognise identities from voice under various conditions. The models trained on our dataset surpass the performance of previous works by a significant margin.}
}

@article{hajibabaei2018unified,
  title={Unified hypersphere embedding for speaker recognition},
  author={Hajibabaei, Mahdi and Dai, Dengxin},
  journal={arXiv preprint arXiv:1807.08312},
  year={2018}
}

@article{chung2020defence,
   title={In Defence of Metric Learning for Speaker Recognition},
   url={http://dx.doi.org/10.21437/Interspeech.2020-1064},
   DOI={10.21437/interspeech.2020-1064},
   journal={Interspeech 2020},
   publisher={ISCA},
   author={Chung, Joon Son and Huh, Jaesung and Mun, Seongkyu and Lee, Minjae and Heo, Hee-Soo and Choe, Soyeon and Ham, Chiheon and Jung, Sunghwan and Lee, Bong-Jin and Han, Icksang},
   year={2020},
   month={Oct} }

@article{chung2018voxceleb2,
   title={VoxCeleb2: Deep Speaker Recognition},
   url={http://dx.doi.org/10.21437/Interspeech.2018-1929},
   DOI={10.21437/interspeech.2018-1929},
   journal={Interspeech 2018},
   publisher={ISCA},
   author={Chung, Joon Son and Nagrani, Arsha and Zisserman, Andrew},
   year={2018},
   month={Sep} }

@article{park2022review,
  title={A review of speaker diarization: Recent advances with deep learning},
  author={Park, Tae Jin and Kanda, Naoyuki and Dimitriadis, Dimitrios and Han, Kyu J and Watanabe, Shinji and Narayanan, Shrikanth},
  journal={Computer Speech \& Language},
  volume={72},
  pages={101317},
  year={2022},
  publisher={Elsevier}
}

@ARTICLE{dehak2011ivectors,
  author={Dehak, Najim and Kenny, Patrick J. and Dehak, Réda and Dumouchel, Pierre and Ouellet, Pierre},
  journal={IEEE Transactions on Audio, Speech, and Language Processing}, 
  title={Front-End Factor Analysis for Speaker Verification}, 
  year={2011},
  volume={19},
  number={4},
  pages={788-798},
  doi={10.1109/TASL.2010.2064307}}

@INPROCEEDINGS{variani2014dvectors,
  author={Variani, Ehsan and Lei, Xin and McDermott, Erik and Moreno, Ignacio Lopez and Gonzalez-Dominguez, Javier},
  booktitle={2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Deep neural networks for small footprint text-dependent speaker verification}, 
  year={2014},
  volume={},
  number={},
  pages={4052-4056},
  doi={10.1109/ICASSP.2014.6854363}}

@inproceedings{snyder2017deep,
  author={David Snyder and Daniel Garcia-Romero and Daniel Povey and Sanjeev Khudanpur},
  title={{Deep Neural Network Embeddings for Text-Independent Speaker Verification}},
  year=2017,
  booktitle={Proc. Interspeech 2017},
  pages={999--1003},
  doi={10.21437/Interspeech.2017-620}
}

@INPROCEEDINGS{snyder2018xvectors,
  author={Snyder, David and Garcia-Romero, Daniel and Sell, Gregory and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={X-Vectors: Robust DNN Embeddings for Speaker Recognition}, 
  year={2018},
  volume={},
  number={},
  pages={5329-5333},
  doi={10.1109/ICASSP.2018.8461375}}

@inproceedings{okabe2018asp,
  author={Koji Okabe and Takafumi Koshinaka and Koichi Shinoda},
  title={{Attentive Statistics Pooling for Deep Speaker Embedding}},
  year=2018,
  booktitle={Proc. Interspeech 2018},
  pages={2252--2256},
  doi={10.21437/Interspeech.2018-993}
}

@inproceedings{peddinti2015timedelay,
  author={Vijayaditya Peddinti and Daniel Povey and Sanjeev Khudanpur},
  title={{A time delay neural network architecture for efficient modeling of long temporal contexts}},
  year=2015,
  booktitle={Proc. Interspeech 2015},
  pages={3214--3218},
  doi={10.21437/Interspeech.2015-647}
}

@ARTICLE{waibel1989timedelay,
  author={Waibel, A. and Hanazawa, T. and Hinton, G. and Shikano, K. and Lang, K.J.},
  journal={IEEE Transactions on Acoustics, Speech, and Signal Processing}, 
  title={Phoneme recognition using time-delay neural networks}, 
  year={1989},
  volume={37},
  number={3},
  pages={328-339},
  doi={10.1109/29.21701}}

@article{cai2018exploring,
  title={Exploring the encoding layer and loss function in end-to-end speaker and language recognition system},
  author={Cai, Weicheng and Chen, Jinkun and Li, Ming},
  journal={arXiv preprint arXiv:1804.05160},
  year={2018}
}

@inproceedings{he2016resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{simonyan2014vgg,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@article{dai2021coatnet,
  title={Coatnet: Marrying convolution and attention for all data sizes},
  author={Dai, Zihang and Liu, Hanxiao and Le, Quoc and Tan, Mingxing},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{chung2019delving,
  title={Delving into voxceleb: environment invariant speaker recognition},
  author={Chung, Joon Son and Huh, Jaesung and Mun, Seongkyu},
  journal={arXiv preprint arXiv:1910.11238},
  year={2019}
}

@article{brummer2013bosaris,
  title={The bosaris toolkit: Theory, algorithms and code for surviving the new dcf},
  author={Br{\"u}mmer, Niko and De Villiers, Edward},
  journal={arXiv preprint arXiv:1304.2865},
  year={2013}
}

@misc{nist2018,
  title = {NIST 2018 Speaker Recognition Evaluation Plan},
  howpublished = {https://www.nist.gov/itl/iad/mig/nist-2018-speaker-recognition-evaluation},
  note = {Accessed: 2022-03-07}
}

@inproceedings{matejka2017asnorm,
  author={Pavel Matějka and Ondřej Novotný and Oldřich Plchot and Lukáš Burget and Mireia Diez Sánchez and Jan Černocký},
  title={{Analysis of Score Normalization in Multilingual Speaker Recognition}},
  year=2017,
  booktitle={Proc. Interspeech 2017},
  pages={1567--1571},
  doi={10.21437/Interspeech.2017-803}
}

@inproceedings{cumani2011comparison,
  author    = {Sandro Cumani and
               Pier Domenico Batzu and
               Daniele Colibro and
               Claudio Vair and
               Pietro Laface and
               Vasileios Vasilakakis},
  title     = {Comparison of Speaker Recognition Approaches for Real Applications},
  booktitle = {{INTERSPEECH} 2011, 12th Annual Conference of the International Speech
               Communication Association, Florence, Italy, August 27-31, 2011},
  pages     = {2365--2368},
  publisher = {{ISCA}},
  year      = {2011},
  url       = {http://www.isca-speech.org/archive/interspeech\_2011/i11\_2365.html},
  timestamp = {Tue, 16 Nov 2021 11:44:29 +0100},
  biburl    = {https://dblp.org/rec/conf/interspeech/CumaniBCVLV11.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{india2019selfmha,
  title={Self multi-head attention for speaker recognition},
  author={India, Miquel and Safari, Pooyan and Hernando, Javier},
  journal={arXiv preprint arXiv:1906.09890},
  year={2019}
}

@inproceedings{liu2019large,
  author={Yi Liu and Liang He and Jia Liu},
  title={{Large Margin Softmax Loss for Speaker Verification}},
  year=2019,
  booktitle={Proc. Interspeech 2019},
  pages={2873--2877},
  doi={10.21437/Interspeech.2019-2357}
}

@inproceedings{deng2019arcface,
  title={Arcface: Additive angular margin loss for deep face recognition},
  author={Deng, Jiankang and Guo, Jia and Xue, Niannan and Zafeiriou, Stefanos},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4690--4699},
  year={2019}
}

@InProceedings{deng2020subarcface,
author="Deng, Jiankang
and Guo, Jia
and Liu, Tongliang
and Gong, Mingming
and Zafeiriou, Stefanos",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="Sub-center ArcFace: Boosting Face Recognition by Large-Scale Noisy Web Faces",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="741--757",
abstract="Margin-based deep face recognition methods (e.g. SphereFace, CosFace, and ArcFace) have achieved remarkable success in unconstrained face recognition. However, these methods are susceptible to the massive label noise in the training data and thus require laborious human effort to clean the datasets. In this paper, we relax the intra-class constraint of ArcFace to improve the robustness to label noise. More specifically, we design K sub-centers for each class and the training sample only needs to be close to any of the K positive sub-centers instead of the only one positive center. The proposed sub-center ArcFace encourages one dominant sub-class that contains the majority of clean faces and non-dominant sub-classes that include hard or noisy faces. Extensive experiments confirm the robustness of sub-center ArcFace under massive real-world noise. After the model achieves enough discriminative power, we directly drop non-dominant sub-centers and high-confident noisy samples, which helps recapture intra-compactness, decrease the influence from noise, and achieve comparable performance compared to ArcFace trained on the manually cleaned dataset. By taking advantage of the large-scale raw web faces (Celeb500K), sub-center Arcface achieves state-of-the-art performance on IJB-B, IJB-C, MegaFace, and FRVT.",
isbn="978-3-030-58621-8"
}